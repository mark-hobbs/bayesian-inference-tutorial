{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b46b220",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo (MCMC) methods\n",
    "\n",
    "Once the posterior is constructed, it needs to be analysed to determine the statistical summaries. For the linear elastic case, the statistical summaries were establised analytically, but for the remaining cases we need to determine them numerically because they are only $C_0$ continuous. We will use a Markov Chain Monte Carlo (MCMC) technique for this.\n",
    "\n",
    "MCMC methods are frequently employed, derivative free numerical approaches to investigate posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49699",
   "metadata": {},
   "source": [
    "## Rejection sampling (accept-reject algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899bbf9",
   "metadata": {},
   "source": [
    "## The standard Metropolis-Hastings algorithm\n",
    "\n",
    "The standard Metropolis-Hastings approach is a frequently employed MCMC algorithm. The basic idea of the Metropolis-Hastings algorithm is to explore the probability density function (PDF) of interest by making a random walk through the parameter space $\\textbf{x}$.\n",
    "\n",
    "### Proposal distribution\n",
    "\n",
    "Consider sample $\\textbf{x}_i$ and its evaluation of the PDF, $\\pi(\\textbf{x}_i)$ new sample $\\textbf{x}_p$ is proposed by drawing from a proposal distribution.\n",
    "\n",
    "$$q(\\textbf{x}_i | \\textbf{x}_p) = q(\\textbf{x}_p | \\textbf{x}_i) \\propto exp\\left(-\\frac{1}{2\\gamma^2}\\|\\textbf{x}_i - \\textbf{x}_p\\|^2\\right)$$\n",
    "\n",
    "$\\gamma$ denotes the parameter that determines the width of the proposal distribution and must be tuned to obtain an efficient and converging algorithm. An efficient starting value is given by the following equation [1]:\n",
    "\n",
    "$$\\gamma = \\frac{2.38}{\\sqrt{n_p}}$$\n",
    "\n",
    "Where $n_p$ denotes the number of unknown parameters.\n",
    "\n",
    "[1] [Gelman A, Roberts GO, Gilks WR (1996) Efficient Metropolis jumping rules. In: Bernardo JM, Berger JO, Dawid AP, Smith AFM (eds) Bayesian Statistics, vol 5. Oxford University Press, pp 599-607](http://people.ee.duke.edu/~lcarin/baystat5.pdf)\n",
    "\n",
    "\n",
    "\n",
    "New sample $\\textbf{x}_p$ is proposed by drawing from a proposal distribution $q(\\textbf{x}_i | \\textbf{x}_p)$... << is this correct?\n",
    "\n",
    "In the case of a symmetric proposal distribution (as in this example), the following relation holds:\n",
    "\n",
    "$$q(\\textbf{x}_i | \\textbf{x}_p) = q(\\textbf{x}_p | \\textbf{x}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ed420",
   "metadata": {},
   "source": [
    "### Visualise the proposal distribution\n",
    "\n",
    "[A Practical Guide to MCMC Part 1: MCMC Basics](https://jellis18.github.io/post/2018-01-02-mcmc-part1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4023b0e4",
   "metadata": {},
   "source": [
    "## The Adaptive Metropolis-Hastings algorithm\n",
    "\n",
    "The width of the proposal distribution $\\gamma$ has to be tuned to obtain an efficient and converging algorithm. It can be seen in the above figures that the chain converges on the correct yield stress $\\sigma_{y0}$ but poor convergence behaviour is observed for the Young's modulus $E$. To overcome the tuning of $\\gamma$, Haario et al. [2] introduced the adaptive proposal (AP) method. The AP method updates the width of the proposal distribution using the existing knowledge of the posterior. The existing knowledge is based on the previous samples.\n",
    "\n",
    "For sample $n_k + 1$, the update employs the following formulation:\n",
    "\n",
    "$$q(\\textbf{x}_p | \\textbf{x}_i) \\sim \\mathcal{N}(\\textbf{x}_i, \\gamma^2 \\textbf{R}_{n_{\\textbf{k}}})$$\n",
    "\n",
    "Where $\\mathcal{N}(\\textbf{x}_i, \\gamma^2 \\textbf{R}_{n_{\\textbf{k}}})$ denotes a normal distribution with mean $\\textbf{x}_i$ and covariance matrix $\\gamma^2 \\textbf{R}_{n_{\\textbf{k}}}$ of size $n_p \\times n_p$. To establish $\\textbf{R}_{n_{\\textbf{k}}}$, all $n_k$ previous samples are first stored in matrix $\\textbf{K}$ of size $n_{\\textbf{K}} \\times n_p$. $\\textbf{R}_{n_{\\textbf{k}}}$ is then computed as:\n",
    "\n",
    "$$\\textbf{R}_{n_{\\textbf{k}}} = \\frac{1}{n_{\\textbf{K}} - 1}\\tilde{\\textbf{K}}^T\\tilde{\\textbf{K}}$$\n",
    "\n",
    "Where $\\tilde{\\textbf{K}} = \\textbf{K} - \\textbf{K}_{mean}$ and $\\textbf{K}_{mean}$ reads:\n",
    "\n",
    "$$\\textbf{K}_{mean} =\n",
    "\\begin{bmatrix}\n",
    "   \\textbf{k}_{mean} \\\\\n",
    "   \\textbf{k}_{mean}  \\\\\n",
    "   \\vdots \\\\\n",
    "   \\textbf{k}_{mean} \n",
    "\\end{bmatrix}_{\\: n_\\textbf{K} \\, \\times \\, n_p}\n",
    "$$\n",
    "\n",
    "and $\\textbf{K}_{mean}$ denotes a row matrix (vector?) of length $n_p$ (number of unknow parameters) which is determined as follows:\n",
    "\n",
    "$$\\textbf{k}_{mean} = \\frac{1}{i}\\begin{bmatrix} \\sum_{i=1}^{n_\\textbf{K}} (K)_{i1} & \\sum_{i=1}^{n_\\textbf{K}} (K)_{i2} & \\cdots & \\sum_{i=1}^{n_\\textbf{K}} (K)_{in_p} \\end{bmatrix}$$\n",
    "\n",
    "The following relation is used for $\\mathcal{N}(\\textbf{x}_i, \\gamma^2 \\textbf{R}_{n_{\\textbf{k}}})$ in this contribution:\n",
    "\n",
    "\n",
    "[2] [Haario, H., Saksman, E., & Tamminen, J. (1999). Adaptive proposal distribution for random walk Metropolis algorithm. Computational Statistics, 14(3), 375-395.](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.3205&rep=rep1&type=pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
